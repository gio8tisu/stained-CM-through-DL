{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a despeckling neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/zhilabs/projects/CM'\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = os.path.join(PATH, 'CM_crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "import tqdm\n",
    "\n",
    "from datasets import NoisyScansDataset\n",
    "from despeckling import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the following variables will help us make our code device agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary we can use to create our noisy dataset.\n",
    "def get_noise_args(noise_type):\n",
    "    if noise_type == 'gaussian':\n",
    "        return {'random_variable': np.random.normal,\n",
    "                      'loc': 1, 'scale': 0.1}\n",
    "    elif noise_type == 'gamma':\n",
    "        return {'random_variable': np.random.gamma,\n",
    "                      'shape': 1, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes the sum of SSIM over a batch of images.\n",
    "def compute_ssim(noisy_batch, clean_batch, median_filter=False):\n",
    "    # iterate over batch to compute SSIM\n",
    "    ssim_sum = 0\n",
    "    for noisy, clean in zip(noisy_batch[:, 0], clean_batch[:, 0]):\n",
    "        noisy = noisy.data.cpu().numpy()\n",
    "\n",
    "        if median_filter:\n",
    "            noisy = (noisy + 1) / 2 * 255\n",
    "            noisy = noisy.astype(np.uint8)\n",
    "            noisy = np.median(noisy)\n",
    "            noisy = (noisy / 255.0 - 0.5) * 2\n",
    "\n",
    "        ssim_sum += ssim(noisy, clean.data.cpu().numpy(), data_range=2)\n",
    "    return ssim_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a torch model based on an easy name\n",
    "def get_model(model_str, num_layers):\n",
    "    if model_str == 'log_add':\n",
    "        return models.LogAddDespeckle(num_layers)\n",
    "    elif model_str == 'log_subtract':\n",
    "        return models.LogSubtractDespeckle(num_layers)\n",
    "    elif model_str == 'multiply':\n",
    "        return models.MultiplyDespeckle(num_layers)\n",
    "    elif model_str == 'divide':\n",
    "        return models.DivideDespeckle(num_layers)\n",
    "    else:\n",
    "        raise NotImplementedError(model_str + 'model does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a pytorch loss object based on a easy name\n",
    "def get_criterion(criterion_str):\n",
    "    if criterion_str == 'mse':\n",
    "        return MSELoss()\n",
    "    elif criterion_str == 'l1':\n",
    "        return L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset returns a pair of images: a multiplicative-noise contaminated image and its corresponding clean image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a 90/10 train/validation split  \n",
    "And contaminate the images with a gamma-distributed noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns (noisy, clean) tuple\n",
    "dataset = NoisyScansDataset(DATA_ROOT, 'F', get_noise_args('gamma'))\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss functions will consist on a distance between the output of the model and the clean image.  \n",
    "L1 (manhattan) or MSE (euclidian) are basic distance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = get_criterion('mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define despeckling model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model consists on a series of convolutional layers followed by a skip connection connected to the input.\n",
    "\n",
    "* We can transform our input image to the log space and use a additive skip connection.\n",
    "* Or use a multiplicative or division connection and work with the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a divide connection with just 2 convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('divide', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss = 0.000: 100%|██████████| 1026/1026 [08:11<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ssim_noisy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b914cacd2933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m' Input loss = {0:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_loss_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m' Input SSIM = {0:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssim_noisy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             + ' Output SSIM = {0:.3f}'.format(ssim_clean / BATCH_SIZE))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ssim_noisy' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "ssim_input = []\n",
    "ssim_output = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # TRAINING.\n",
    "    model.train()\n",
    "\n",
    "    print('Epoch {} of {}'.format(epoch, EPOCHS - 1))\n",
    "    input_and_target = tqdm.tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "\n",
    "    for i, (x_batch, target_batch) in input_and_target:\n",
    "        x_batch, target_batch = x_batch.float().to(device), target_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_batch = model(x_batch)\n",
    "\n",
    "        loss = criterion(output_batch, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.data.cpu().numpy())\n",
    "\n",
    "        input_and_target.set_description('Train loss = {0:.3f}'.format(loss))\n",
    "\n",
    "    # VALIDATION.\n",
    "    print('Validation:')\n",
    "    model.eval()\n",
    "\n",
    "    input_and_target = tqdm.tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
    "\n",
    "    med_loss_eval = 0\n",
    "    prev_loss_eval = 0\n",
    "    for i, (x_batch, target_batch) in input_and_target:\n",
    "        x_batch, target_batch = x_batch.float().to(device), target_batch.float().to(device)\n",
    "        output_batch = model(x_batch)\n",
    "        loss = criterion(output_batch, target_batch)\n",
    "        med_loss_eval += loss.data.cpu().numpy()\n",
    "        prev_loss_eval = criterion(x_batch, target_batch).data.cpu().numpy()\n",
    "\n",
    "        ssim_input.append(compute_ssim(x_batch, target_batch) / BATCH_SIZE)\n",
    "        ssim_output.append(compute_ssim(output_batch, target_batch) / BATCH_SIZE)\n",
    "        val_loss.append(loss)\n",
    "            \n",
    "        input_and_target.set_description(\n",
    "            'Output loss = {0:.3f}'.format(loss)\n",
    "            + ' Input loss = {0:.3f}'.format(prev_loss_eval)\n",
    "            + ' Input SSIM = {0:.3f}'.format(ssim_input[-1])\n",
    "            + ' Output SSIM = {0:.3f}'.format(ssim_input[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
