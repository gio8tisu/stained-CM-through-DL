{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a despeckling neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "import tqdm\n",
    "\n",
    "from datasets import NoisyScansDataset\n",
    "from despeckling import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the following variables will help us make our code device agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary we can use to create our noisy dataset.\n",
    "def get_noise_args(noise_type):\n",
    "    if noise_type == 'gaussian':\n",
    "        noise_args = {'random_variable': np.random.normal,\n",
    "                      'loc': 1, 'scale': 0.1}\n",
    "    elif noise_type == 'gamma':\n",
    "        noise_args = {'random_variable': np.random.gamma,\n",
    "                      'shape': 1, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes the sum of SSIM over a batch of images.\n",
    "def compute_ssim(noisy_batch, clean_batch, median_filter=False):\n",
    "    # iterate over batch to compute SSIM\n",
    "    ssim_sum = 0\n",
    "    for noisy, clean in zip(noisy_batch[:, 0], clean_batch[:, 0]):\n",
    "        noisy = noisy.data.cpu().numpy()\n",
    "\n",
    "        if median_filter:\n",
    "            noisy = (noisy + 1) / 2 * 255\n",
    "            noisy = noisy.astype(np.uint8)\n",
    "            noisy = np.median(noisy)\n",
    "            noisy = (noisy / 255.0 - 0.5) * 2\n",
    "\n",
    "        ssim_sum += ssim(noisy, clean.data.cpu().numpy(), data_range=2)\n",
    "    return ssim_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a torch model based on an easy name\n",
    "def get_model(model_str, num_layers):\n",
    "    if model_str == 'log_add':\n",
    "        return models.LogAddDespeckle(num_layers)\n",
    "    elif model_str == 'log_subtract':\n",
    "        return models.LogSubtractDespeckle(num_layers)\n",
    "    elif model_str == 'multiply':\n",
    "        return models.MultiplyDespeckle(num_layers)\n",
    "    elif model_str == 'divide':\n",
    "        return models.DivideDespeckle(num_layers)\n",
    "    else:\n",
    "        raise NotImplementedError(model_str + 'model does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a pytorch loss object based on a easy name\n",
    "def get_criterion(criterion_str):\n",
    "    if criterion_str == 'mse':\n",
    "        criterion = MSELoss()\n",
    "    elif criterion_str == 'l1':\n",
    "        criterion = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset returns a pair of images: a multiplicative-noise contaminated image and its corresponding clean image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a 90/10 train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns (noisy, clean) tuple\n",
    "dataset = NoisyScansDataset(args.data_root, 'F', noise_args, apply_random_crop=(not args.no_crop))\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss functions will consist on a distance between the output of the model and the clean image.  \n",
    "L1 (manhattan) or MSE (euclidian) are basic distance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = get_criterion('mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define despeckling model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model consists on a series of convolutional layers followed by a skip connection connected to the input.\n",
    "\n",
    "* We can transform our input image to the log space and use a additive skip connection.\n",
    "* Or use a multiplicative or division connection and work with the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('divide', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    # TRAINING.\n",
    "    model.train()\n",
    "\n",
    "    print('Epoch {} of {}'.format(epoch, args.epochs - 1))\n",
    "    input_and_target = tqdm.tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "\n",
    "    med_loss = 0\n",
    "    for i, (x_batch, target_batch) in input_and_target:\n",
    "        x_batch, target_batch = x_batch.float().to(device), target_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_batch = model(x_batch)\n",
    "\n",
    "        loss = criterion(output_batch, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        med_loss += loss.data.cpu().numpy()\n",
    "\n",
    "        input_and_target.set_description('Train loss = {0:.3f}'.format(loss))\n",
    "\n",
    "    # VALIDATION.\n",
    "    print('Validation:')\n",
    "    model.eval()\n",
    "\n",
    "    input_and_target = tqdm.tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
    "\n",
    "    med_loss_eval = 0\n",
    "    prev_loss_eval = 0\n",
    "    for i, (x_batch, target_batch) in input_and_target:\n",
    "        x_batch, target_batch = x_batch.float().to(device), target_batch.float().to(device)\n",
    "        output_batch = model(x_batch)\n",
    "        loss = criterion(output_batch, target_batch)\n",
    "        med_loss_eval += loss.data.cpu().numpy()\n",
    "        prev_loss_eval = criterion(x_batch, target_batch).data.cpu().numpy()\n",
    "\n",
    "        ssim_input = compute_ssim(x_batch, target_batch)\n",
    "        ssim_output = compute_ssim(output_batch, target_batch)\n",
    "            \n",
    "        input_and_target.set_description(\n",
    "            'Output loss = {0:.3f}'.format(loss)\n",
    "            + ' Input loss = {0:.3f}'.format(prev_loss_eval)\n",
    "            + ' Input SSIM = {0:.3f}'.format(ssim_noisy / args.batch_size)\n",
    "            + ' Output SSIM = {0:.3f}'.format(ssim_clean / args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
